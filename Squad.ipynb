{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/eshwar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Directly input your Hugging Face token here\n",
    "huggingface_token = \"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\"\n",
    "\n",
    "# Log in using the token\n",
    "login(token=huggingface_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the SQuAD v2 dataset\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "print(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 142192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Combine train and validation splits\n",
    "combined_squad = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "\n",
    "# Display the structure of the combined dataset\n",
    "print(combined_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 113753\n",
      "})\n",
      "Test Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 28439\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the combined dataset into train and test (80:20 split)\n",
    "combined_squad = combined_squad.train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "# Create train and test splits\n",
    "train_squad = combined_squad['train']\n",
    "test_squad = combined_squad['test']\n",
    "\n",
    "# Display the structure of the splits\n",
    "print(f\"Train Split: {train_squad}\")\n",
    "print(f\"Test Split: {test_squad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_qa(examples):\n",
    "    # Strip spaces from questions\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    # Tokenize questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Loop through each offset mapping\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        # Prevent out-of-range access for answers\n",
    "        if i >= len(examples[\"answers\"]):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Safeguard for empty answers\n",
    "        if len(examples[\"answers\"][i][\"text\"]) > 0:  # Check if there is an answer\n",
    "            # Extract answer details\n",
    "            answer_text = examples[\"answers\"][i][\"text\"][0]\n",
    "            answer_start = examples[\"answers\"][i][\"answer_start\"][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Find token start and end positions\n",
    "            token_start = token_end = None\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start <= answer_start < end:\n",
    "                    token_start = idx\n",
    "                if start < answer_end <= end:\n",
    "                    token_end = idx\n",
    "                    break\n",
    "\n",
    "            # Assign positions or default to CLS token index (0)\n",
    "            start_positions.append(token_start if token_start is not None else 0)\n",
    "            end_positions.append(token_end if token_end is not None else 0)\n",
    "        else:\n",
    "            # Assign default positions for empty answers\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "    # Add start and end positions to the tokenized inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    \n",
    "    # Remove offset mapping to save memory\n",
    "    inputs.pop(\"offset_mapping\", None)\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe7da774cd34f70be48d2fb9e95fbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4548c13f40476fb78d6770cc07813f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Train Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 113753\n",
      "})\n",
      "Preprocessed Test Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 28439\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_squad_tokenized = train_squad.map(\n",
    "    preprocess_qa, batched=True, batch_size=100,\n",
    ")\n",
    "test_squad_tokenized = test_squad.map(\n",
    "    preprocess_qa, batched=True, batch_size=100,\n",
    ")\n",
    "\n",
    "# Print structure\n",
    "print(f\"Preprocessed Train Split: {train_squad_tokenized}\")\n",
    "print(f\"Preprocessed Test Split: {test_squad_tokenized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/eshwar/anaconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /home/eshwar/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (0.25.2)\n",
      "Requirement already satisfied: packaging in /home/eshwar/anaconda3/lib/python3.12/site-packages (from evaluate) (23.2)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/eshwar/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/eshwar/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /home/eshwar/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/eshwar/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eshwar/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a274cdcde33f25aeba4b4a3968bb0e4f7c458638e05db82c6f3f7b3b504b17d2\n",
      "  Stored in directory: /home/eshwar/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.1.0 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4453990aedd44709e13acfb10e46497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2c708722964073a078b90cf145ae9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf3c654e86b4fa1a6196a42939d5048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608743d8a0774e18b1010230a8e9d6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b04209c62c847e088f362f140965d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50da8ed78854e92a05c556bc1237651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d69d1873b434acbb139a7ddf7ede404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/eshwar/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /home/eshwar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/eshwar/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the metrics\n",
    "squad = evaluate.load(\"squad_v2\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "# Function to compute the metrics\n",
    "def compute_metrics(pred):\n",
    "    # Get the true labels and predictions\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "\n",
    "    # Decode the predictions and labels (the tokenizer will help with this)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute exact match\n",
    "    exact_match = squad.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute F1 score (using sklearn)\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute ROUGE score\n",
    "    rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute METEOR score\n",
    "    meteor_score_value = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": exact_match[\"exact_match\"],\n",
    "        \"f1\": f1,\n",
    "        \"bleu\": bleu_score[\"bleu\"],\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"meteor\": meteor_score_value,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to put model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set up logging to output to console\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Define the training arguments with logging at the end of each epoch\n",
    "training_args_qa = TrainingArguments(\n",
    "    output_dir=\"./results_squad\",  # Directory where results will be saved\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    save_strategy=\"epoch\",        # Save checkpoints at the end of every epoch\n",
    "    learning_rate=2e-5,           # Learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,   # Batch size for evaluation\n",
    "    num_train_epochs=3,             # Number of training epochs\n",
    "    weight_decay=0.01,              # Weight decay to prevent overfitting\n",
    "    logging_dir=\"./logs\",           # Directory for logging\n",
    "    logging_strategy=\"epoch\",      # Log at the end of each epoch\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer_qa = Trainer(\n",
    "    model=model_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=encoded_squad[\"train\"],  # Train dataset\n",
    "    eval_dataset=encoded_squad[\"test\"],   # Test dataset\n",
    "    tokenizer=tokenizer,                  # Use the tokenizer for encoding\n",
    "    data_collator=default_data_collator,  # Handles padding for variable-length sequences\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "# Load the base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExtractiveQAModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ExtractiveQAModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.qa_outputs = nn.Linear(base_model.config.hidden_size, 2)  # Start and end logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Get hidden states from the base model\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Compute start and end logits\n",
    "        logits = self.qa_outputs(hidden_states)  # Shape: (batch_size, seq_length, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # Shape: (batch_size, seq_length, 1)\n",
    "        start_logits = start_logits.squeeze(-1)  # Shape: (batch_size, seq_length)\n",
    "        end_logits = end_logits.squeeze(-1)      # Shape: (batch_size, seq_length)\n",
    "        \n",
    "        return start_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_qa = ExtractiveQAModel(base_model)\n",
    "model_qa.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with logging at the end of each epoch\n",
    "trainer_qa.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model after training\n",
    "results_qa = trainer_qa.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results_qa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
