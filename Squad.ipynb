{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/eshwar/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Directly input your Hugging Face token here\n",
    "huggingface_token = \"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\"\n",
    "\n",
    "# Log in using the token\n",
    "login(token=huggingface_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the SQuAD v2 dataset\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "print(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 142192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Combine train and validation splits\n",
    "combined_squad = concatenate_datasets([squad_dataset['train'], squad_dataset['validation']])\n",
    "\n",
    "# Display the structure of the combined dataset\n",
    "print(combined_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 113753\n",
      "})\n",
      "Test Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 28439\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the combined dataset into train and test (80:20 split)\n",
    "combined_squad = combined_squad.train_test_split(test_size=0.2, seed=1)\n",
    "\n",
    "# Create train and test splits\n",
    "train_squad = combined_squad['train']\n",
    "test_squad = combined_squad['test']\n",
    "\n",
    "# Display the structure of the splits\n",
    "print(f\"Train Split: {train_squad}\")\n",
    "print(f\"Test Split: {test_squad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_qa(examples):\n",
    "    # Strip spaces from questions\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    # Tokenize questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Loop through each offset mapping\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        # Prevent out-of-range access for answers\n",
    "        if i >= len(examples[\"answers\"]):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Safeguard for empty answers\n",
    "        if len(examples[\"answers\"][i][\"text\"]) > 0:  # Check if there is an answer\n",
    "            # Extract answer details\n",
    "            answer_text = examples[\"answers\"][i][\"text\"][0]\n",
    "            answer_start = examples[\"answers\"][i][\"answer_start\"][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Find token start and end positions\n",
    "            token_start = token_end = None\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start <= answer_start < end:\n",
    "                    token_start = idx\n",
    "                if start < answer_end <= end:\n",
    "                    token_end = idx\n",
    "                    break\n",
    "\n",
    "            # Assign positions or default to CLS token index (0)\n",
    "            start_positions.append(token_start if token_start is not None else 0)\n",
    "            end_positions.append(token_end if token_end is not None else 0)\n",
    "        else:\n",
    "            # Assign default positions for empty answers\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "\n",
    "    # Add start and end positions to the tokenized inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    \n",
    "    # Remove offset mapping to save memory\n",
    "    inputs.pop(\"offset_mapping\", None)\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe7da774cd34f70be48d2fb9e95fbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4548c13f40476fb78d6770cc07813f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Train Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 113753\n",
      "})\n",
      "Preprocessed Test Split: Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 28439\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_squad_tokenized = train_squad.map(\n",
    "    preprocess_qa, batched=True, batch_size=100,\n",
    ")\n",
    "test_squad_tokenized = test_squad.map(\n",
    "    preprocess_qa, batched=True, batch_size=100,\n",
    ")\n",
    "\n",
    "# Print structure\n",
    "print(f\"Preprocessed Train Split: {train_squad_tokenized}\")\n",
    "print(f\"Preprocessed Test Split: {test_squad_tokenized}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
